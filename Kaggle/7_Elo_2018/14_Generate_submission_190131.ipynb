{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission based on bag of catboost classifiers\n",
    "### 31-01-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables specific for competition\n",
    "\n",
    "ID = 'card_id'                                            \n",
    "TARGET = 'target'    \n",
    "\n",
    "RAW_DIRECTORY = 'C:/Users/judit/Documents/learning/kaggle/Elo_201812/rawdata/'  \n",
    "DIRECTORY = 'C:/Users/judit/Documents/learning/kaggle/Elo_201812/data/'\n",
    "HIST_TRANS_FILE = RAW_DIRECTORY + 'historical_transactions.csv'\n",
    "MERCHANTS_FILE = RAW_DIRECTORY + 'merchants.csv'\n",
    "NEW_MERCH_TRANS_FILE = RAW_DIRECTORY + 'new_merchant_transactions.csv'\n",
    "TRAIN_FILE = RAW_DIRECTORY + 'train.csv'    \n",
    "TEST_FILE = RAW_DIRECTORY +'test.csv'\n",
    "SAMPLE_SUBMISSION_FILE = RAW_DIRECTORY + 'sample_submission.csv'\n",
    "\n",
    "SUBMISSION_DIRECTORY = 'C:/Users/judit/Documents/learning/kaggle/Elo_201812/submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables specific for notebook\n",
    "MODELS_FILE = DIRECTORY + 'models_12_notebook.p'\n",
    "PREDICTIONS_CLASS_FILE = DIRECTORY + 'predictions_class_12_notebook_1.p'\n",
    "PREDICTIONS_PROBA_FILE = DIRECTORY + 'predictions_proba_12_notebook_1.p'\n",
    "LOCALTEST_SET_FILE = DIRECTORY + 'localtest_set_12_notebook.p'\n",
    "\n",
    "NUM = 0\n",
    "SUBMIT_FILENAME = SUBMISSION_DIRECTORY + 'submit_190131_'\n",
    "\n",
    "HIST_AGG_FILE_1 = DIRECTORY + 'hist_agg_20.pkl'        # only authorized, domestic, last 3 months\n",
    "HIST_AGG_FILE_2 = DIRECTORY + 'hist_agg_28.pkl'        # only authorized, non-domestic, last 3 months\n",
    "NEW_AGG_FILE_1 = DIRECTORY + 'new_agg_3.pkl'           # only authorized, domestic, whole period\n",
    "NEW_AGG_FILE_2 = DIRECTORY + 'new_agg_5.pkl'           # only authorized, non-domestic, whole period\n",
    "\n",
    "categorical_features = ['first_active_month',\n",
    "                        'card_id',\n",
    "                        'feature_1',\n",
    "                        'feature_2',\n",
    "                        'feature_3',\n",
    "                        'hist_city_id_mode_auth_dom_lag2m',\n",
    "                        'hist_category_3_mode_auth_dom_lag2m',\n",
    "                        'hist_merchant_category_id_mode_auth_dom_lag2m',\n",
    "                        'hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'hist_category_2_mode_auth_dom_lag2m',\n",
    "                        'hist_state_id_mode_auth_dom_lag2m',\n",
    "                        'hist_subsector_id_mode_auth_dom_lag2m',\n",
    "                        'hist_city_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_category_3_mode_auth_nondom_lag2m',\n",
    "                        'hist_merchant_category_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_category_2_mode_auth_nondom_lag2m',\n",
    "                        'hist_state_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_subsector_id_mode_auth_nondom_lag2m',\n",
    "                        'new_city_id_mode_dom_all',\n",
    "                        'new_category_3_mode_dom_all',\n",
    "                        'new_merchant_category_id_mode_dom_all',\n",
    "                        'new_merchant_id_mode_dom_all',\n",
    "                        'new_category_2_mode_dom_all',\n",
    "                        'new_state_id_mode_dom_all',\n",
    "                        'new_subsector_id_mode_dom_all',\n",
    "                        'new_city_id_mode_nondom_all',\n",
    "                        'new_category_3_mode_nondom_all',\n",
    "                        'new_merchant_category_id_mode_nondom_all',\n",
    "                        'new_merchant_id_mode_nondom_all',\n",
    "                        'new_category_2_mode_nondom_all',\n",
    "                        'new_state_id_mode_nondom_all',\n",
    "                        'new_subsector_id_mode_nondom_all',\n",
    "                        'merchant_group_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'merchant_category_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'subsector_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'city_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'state_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'category_2_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'merchant_group_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'merchant_category_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'subsector_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'city_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'state_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'category_2_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'merchant_group_id_new_merchant_id_mode_dom_all',\n",
    "                        'merchant_category_id_new_merchant_id_mode_dom_all',\n",
    "                        'subsector_id_new_merchant_id_mode_dom_all',\n",
    "                        'city_id_new_merchant_id_mode_dom_all',\n",
    "                        'state_id_new_merchant_id_mode_dom_all',\n",
    "                        'category_2_new_merchant_id_mode_dom_all',\n",
    "                        'merchant_group_id_new_merchant_id_mode_nondom_all',\n",
    "                        'merchant_category_id_new_merchant_id_mode_nondom_all',\n",
    "                        'subsector_id_new_merchant_id_mode_nondom_all',\n",
    "                        'city_id_new_merchant_id_mode_nondom_all',\n",
    "                        'state_id_new_merchant_id_mode_nondom_all',\n",
    "                        'category_2_new_merchant_id_mode_nondom_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, log_loss, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "random.seed(1)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful functions\n",
    "def reduce_mem_usage(df, verbose = True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem\\\n",
    "                                                                                                   ) / start_mem))\n",
    "    return df\n",
    "\n",
    "def mode(series):\n",
    "    if len(series.mode()) > 0:\n",
    "        return series.mode().iloc[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def label_encoder(train_df, test_df = None, valid_df = None, localtest_df = None,\n",
    "                  prefix = '_labelenc_', suffix = '',\n",
    "                  target = 'is_outlier',\n",
    "                  cols_and_encodings = {'feature_1' : ['mean', 'median'],\n",
    "                                         'feature_2' : [mode]}):\n",
    "    '''\n",
    "    Calculates label encodings based on train_df.\n",
    "    Can be used both for classification and regression problems.\n",
    "    \n",
    "    cols_and_encodings is a dictionary. \n",
    "    Keys: features that we want to encode (usually categorical features with only a few possible values).\n",
    "    Values : list of aggregation functions which will be applied on target values.\n",
    "    '''\n",
    "    for col, enc_list in cols_and_encodings.items():\n",
    "        agg_df = train_df.groupby(col).agg({target : enc_list})\n",
    "        agg_df.columns = [col + prefix + '_'.join(colname).strip() + suffix for colname in agg_df.columns.values]  \n",
    "        agg_df.reset_index(inplace = True)\n",
    "        train_df = train_df.merge(agg_df, how = 'left', on = col)\n",
    "        if test_df:\n",
    "            test_df = test_df.merge(agg_df, how = 'left', on = col)\n",
    "        if valid_df:\n",
    "            valid_df = valid_df.merge(agg_df, how = 'left', on = col)\n",
    "        if localtest_df:\n",
    "            localtest_df = localtest_df.merge(agg_df, how = 'left', on = col)\n",
    "\n",
    "    return train_df, test_df, valid_df, localtest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3\n",
       "0         2017-04-01  C_ID_0ab67a22ab          3          3          1\n",
       "1         2017-01-01  C_ID_130fd0cbdd          2          3          0\n",
       "2         2017-08-01  C_ID_b709037bc5          5          1          1\n",
       "3         2017-12-01  C_ID_d27d835a9f          2          1          0\n",
       "4         2015-12-01  C_ID_2b5e3df5c2          5          1          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_agg_1 = pd.read_pickle(HIST_AGG_FILE_1)\n",
    "hist_agg_2 = pd.read_pickle(HIST_AGG_FILE_2)\n",
    "new_agg_1 = pd.read_pickle(NEW_AGG_FILE_1)\n",
    "new_agg_2 = pd.read_pickle(NEW_AGG_FILE_2)\n",
    "merch = pd.read_csv(MERCHANTS_FILE)\n",
    "test = pd.read_csv(TEST_FILE, parse_dates = [\"first_active_month\"])\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_transformations_and_merges():\n",
    "    '''\n",
    "    This function is similar to function data_transformations_and_merges() in notebook 12, but now we don't create\n",
    "    'is_outlier' column.\n",
    "    '''\n",
    "    global test, merch\n",
    "    # card_id is a hexadecimal number. Convert it into decimal.\n",
    "    test['card_id'] = test['card_id'].apply(lambda s : int(s[5:], 16))\n",
    "    test = reduce_mem_usage(df = test, verbose = True)\n",
    "    \n",
    "    # merchant_id is a hexadecimal number. Convert it into decimal.\n",
    "    merch['merchant_id'] = merch['merchant_id'].apply(lambda s : int(s[5:], 16))\n",
    "    # convert categorical features into numerical ones\n",
    "    merch['category_1'] = merch['category_1'].apply(lambda x : 1 if x == 'Y' else 0 if x == 'N' else np.nan)\n",
    "    merch['most_recent_sales_range'] = merch['most_recent_sales_range'].apply(lambda x : 1 if x == 'A' else\n",
    "                                                                                         2 if x == 'B' else\n",
    "                                                                                         3 if x == 'C' else\n",
    "                                                                                         4 if x == 'D' else\n",
    "                                                                                         5 if x == 'E' else\n",
    "                                                                                         np.nan)\n",
    "    merch['most_recent_purchases_range'] = merch['most_recent_purchases_range'].apply(lambda x : 1 if x == 'A' else\n",
    "                                                                                                 2 if x == 'B' else\n",
    "                                                                                                 3 if x == 'C' else\n",
    "                                                                                                 4 if x == 'D' else\n",
    "                                                                                                 5 if x == 'E' else\n",
    "                                                                                                 np.nan)\n",
    "    merch['category_4'] = merch['category_4'].apply(lambda x : 1 if x == 'Y' else 0 if x == 'N' else np.nan)\n",
    "    merch = reduce_mem_usage(df = merch, verbose = True)\n",
    "    \n",
    "    # merges\n",
    "    df = test.merge(hist_agg_1, how = 'left', on = 'card_id')\n",
    "    df = df.merge(hist_agg_2, how = 'left', on = 'card_id')\n",
    "    df = df.merge(new_agg_1, how = 'left', on = 'card_id')\n",
    "    df = df.merge(new_agg_2, how = 'left', on = 'card_id')\n",
    "    df = df.merge(merch, how = 'left', left_on = 'hist_merchant_id_mode_auth_dom_lag2m', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_hist_merchant_id_mode_auth_dom_lag2m'))\n",
    "    df = df.merge(merch, how = 'left', left_on = 'hist_merchant_id_mode_auth_nondom_lag2m', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_hist_merchant_id_mode_auth_nondom_lag2m'))\n",
    "    df = df.merge(merch, how = 'left', left_on = 'new_merchant_id_mode_dom_all', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_new_merchant_id_mode_dom_all'))\n",
    "    df = df.merge(merch, how = 'left', left_on = 'new_merchant_id_mode_nondom_all', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_new_merchant_id_mode_nondom_all'))\n",
    "    for colname in merch.columns:\n",
    "        df.rename(columns = {colname: colname + '_hist_merchant_id_mode_auth_dom_lag2m'}, inplace = True)\n",
    "    df.drop(['merchant_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "             'merchant_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "             'merchant_id_new_merchant_id_mode_dom_all',\n",
    "             'merchant_id_new_merchant_id_mode_nondom_all'], inplace = True, axis = 1)\n",
    "    \n",
    "    # Prepair data for CatBoostClassifier\n",
    "    # impute string for missing categorical values (catboost can't handle np.nan in categorical features)\n",
    "    df[categorical_features] = df[categorical_features].fillna('nan')\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        df[col] = df[col].astype('category').cat.codes\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  4.72 MB\n",
      "Reduced memory usage:  2.24 MB (52.5% reduction)\n",
      "Starting memory usage: 56.18 MB\n",
      "Reduced memory usage: 21.39 MB (61.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "df = test_data_transformations_and_merges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_of_predictions(models_list, test_set):\n",
    "    predictions_class = []\n",
    "    predictions_proba = []\n",
    "    idx = 0\n",
    "    for model in models_list:\n",
    "        print('Predict for model', idx)\n",
    "        prediction_class = model.predict(test_set)\n",
    "        prediction_proba = model.predict_proba(test_set)\n",
    "        predictions_class.append(prediction_class)\n",
    "        predictions_proba.append(prediction_proba)\n",
    "        idx += 1\n",
    "    return predictions_class, predictions_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict for model 0\n",
      "Predict for model 1\n",
      "Predict for model 2\n",
      "Predict for model 3\n",
      "Predict for model 4\n",
      "Predict for model 5\n",
      "Predict for model 6\n",
      "Predict for model 7\n",
      "Predict for model 8\n",
      "Predict for model 9\n",
      "Predict for model 10\n",
      "Predict for model 11\n",
      "Predict for model 12\n",
      "Predict for model 13\n",
      "Predict for model 14\n",
      "Predict for model 15\n",
      "Predict for model 16\n",
      "Predict for model 17\n",
      "Predict for model 18\n",
      "Predict for model 19\n",
      "Predict for model 20\n",
      "Predict for model 21\n",
      "Predict for model 22\n",
      "Predict for model 23\n",
      "Predict for model 24\n",
      "Predict for model 25\n",
      "Predict for model 26\n",
      "Predict for model 27\n",
      "Predict for model 28\n",
      "Predict for model 29\n",
      "Predict for model 30\n",
      "Predict for model 31\n"
     ]
    }
   ],
   "source": [
    "models = pickle.load(open(DIRECTORY + 'models_12_notebook.p', 'rb'))\n",
    "\n",
    "predictions_class, predictions_proba = make_list_of_predictions(models_list = models, test_set = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123623, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix of predictions for probabilities of the positive class:\n",
    "all_preds = np.array(predictions_proba)[:,:,1].T\n",
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pickle.load(open(DIRECTORY + 'best_catboost_weights_13_notebook.p', 'rb'))\n",
    "thr = 0.9\n",
    "\n",
    "weighted_prediction = np.sum(np.multiply(all_preds, weights) / np.sum(weights), axis = 1)\n",
    "preds_class = np.array([(weighted_prediction > thr).astype(int)]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1321.0123896598816\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print('Elapsed time:', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of positive predictions\n",
    "sum(preds_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_class[preds_class == 1] = -33.219281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id  target\n",
       "0  C_ID_0ab67a22ab       0\n",
       "1  C_ID_130fd0cbdd       0\n",
       "2  C_ID_b709037bc5       0\n",
       "3  C_ID_d27d835a9f       0\n",
       "4  C_ID_2b5e3df5c2       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "submission['target'] = preds_class\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 0\n",
    "submission.to_csv(SUBMIT_FILENAME + str(NUM) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
