{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Competition\n",
    "## Isolation Forest\n",
    "### Outlier Detection\n",
    "#### 03-02-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables specific for competition\n",
    "\n",
    "ID = 'card_id'                                            \n",
    "TARGET = 'target'    \n",
    "\n",
    "RAW_DIRECTORY = 'C:/Users/judit/Documents/learning/kaggle/Elo_201812/rawdata/'  \n",
    "DIRECTORY = 'C:/Users/judit/Documents/learning/kaggle/Elo_201812/data/'\n",
    "HIST_TRANS_FILE = RAW_DIRECTORY + 'historical_transactions.csv'\n",
    "MERCHANTS_FILE = RAW_DIRECTORY + 'merchants.csv'\n",
    "NEW_MERCH_TRANS_FILE = RAW_DIRECTORY + 'new_merchant_transactions.csv'\n",
    "TRAIN_FILE = RAW_DIRECTORY + 'train.csv'    \n",
    "TEST_FILE = RAW_DIRECTORY +'test.csv'\n",
    "SAMPLE_SUBMISSION_FILE = RAW_DIRECTORY + 'sample_submission.csv'\n",
    "\n",
    "SUBMISSION_DIRECTORY = 'C:/Users/judit/Documents/learning/kaggle/Elo_201812/submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variables specific for notebook\n",
    "NUM = 0\n",
    "SUBMIT_FILENAME = SUBMISSION_DIRECTORY + 'iso_forest_submit_190104_'\n",
    "\n",
    "HIST_AGG_FILE_1 = DIRECTORY + 'hist_agg_20.pkl'        # only authorized, domestic, last 3 months\n",
    "HIST_AGG_FILE_2 = DIRECTORY + 'hist_agg_28.pkl'        # only authorized, non-domestic, last 3 months\n",
    "NEW_AGG_FILE_1 = DIRECTORY + 'new_agg_3.pkl'           # only authorized, domestic, whole period\n",
    "NEW_AGG_FILE_2 = DIRECTORY + 'new_agg_5.pkl'           # only authorized, non-domestic, whole period\n",
    "\n",
    "categorical_features = ['first_active_month',\n",
    "                        'card_id',\n",
    "           #             'feature_1',\n",
    "           #             'feature_2',\n",
    "           #             'feature_3',\n",
    "                        'hist_city_id_mode_auth_dom_lag2m',\n",
    "                        'hist_category_3_mode_auth_dom_lag2m',\n",
    "                        'hist_merchant_category_id_mode_auth_dom_lag2m',\n",
    "                        'hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'hist_category_2_mode_auth_dom_lag2m',\n",
    "                        'hist_state_id_mode_auth_dom_lag2m',\n",
    "                        'hist_subsector_id_mode_auth_dom_lag2m',\n",
    "                        'hist_city_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_category_3_mode_auth_nondom_lag2m',\n",
    "                        'hist_merchant_category_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_category_2_mode_auth_nondom_lag2m',\n",
    "                        'hist_state_id_mode_auth_nondom_lag2m',\n",
    "                        'hist_subsector_id_mode_auth_nondom_lag2m',\n",
    "                        'new_city_id_mode_dom_all',\n",
    "                        'new_category_3_mode_dom_all',\n",
    "                        'new_merchant_category_id_mode_dom_all',\n",
    "                        'new_merchant_id_mode_dom_all',\n",
    "                        'new_category_2_mode_dom_all',\n",
    "                        'new_state_id_mode_dom_all',\n",
    "                        'new_subsector_id_mode_dom_all',\n",
    "                        'new_city_id_mode_nondom_all',\n",
    "                        'new_category_3_mode_nondom_all',\n",
    "                        'new_merchant_category_id_mode_nondom_all',\n",
    "                        'new_merchant_id_mode_nondom_all',\n",
    "                        'new_category_2_mode_nondom_all',\n",
    "                        'new_state_id_mode_nondom_all',\n",
    "                        'new_subsector_id_mode_nondom_all',\n",
    "                        'merchant_group_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'merchant_category_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'subsector_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'city_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'state_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'category_2_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "                        'merchant_group_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'merchant_category_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'subsector_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'city_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'state_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'category_2_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "                        'merchant_group_id_new_merchant_id_mode_dom_all',\n",
    "                        'merchant_category_id_new_merchant_id_mode_dom_all',\n",
    "                        'subsector_id_new_merchant_id_mode_dom_all',\n",
    "                        'city_id_new_merchant_id_mode_dom_all',\n",
    "                        'state_id_new_merchant_id_mode_dom_all',\n",
    "                        'category_2_new_merchant_id_mode_dom_all',\n",
    "                        'merchant_group_id_new_merchant_id_mode_nondom_all',\n",
    "                        'merchant_category_id_new_merchant_id_mode_nondom_all',\n",
    "                        'subsector_id_new_merchant_id_mode_nondom_all',\n",
    "                        'city_id_new_merchant_id_mode_nondom_all',\n",
    "                        'state_id_new_merchant_id_mode_nondom_all',\n",
    "                        'category_2_new_merchant_id_mode_nondom_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\gbms\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "random.seed(1)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful functions\n",
    "def reduce_mem_usage(df, verbose = True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem\\\n",
    "                                                                                                   ) / start_mem))\n",
    "    return df\n",
    "\n",
    "def mode(series):\n",
    "    if len(series.mode()) > 0:\n",
    "        return series.mode().iloc[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def label_encoder(train_df, test_df = None, valid_df = None, localtest_df = None,\n",
    "                  prefix = '_labelenc_', suffix = '',\n",
    "                  target = 'is_outlier',\n",
    "                  cols_and_encodings = {'feature_1' : ['mean', 'median'],\n",
    "                                         'feature_2' : [mode]}):\n",
    "    '''\n",
    "    Calculates label encodings based on train_df.\n",
    "    Can be used both for classification and regression problems.\n",
    "    \n",
    "    cols_and_encodings is a dictionary. \n",
    "    Keys: features that we want to encode (usually categorical features with only a few possible values).\n",
    "    Values : list of aggregation functions which will be applied on target values.\n",
    "    '''\n",
    "    for col, enc_list in cols_and_encodings.items():\n",
    "        agg_df = train_df.groupby(col).agg({target : enc_list})\n",
    "        agg_df.columns = [col + prefix + '_'.join(colname).strip() + suffix for colname in agg_df.columns.values]  \n",
    "        agg_df.reset_index(inplace = True)\n",
    "        train_df = train_df.merge(agg_df, how = 'left', on = col)\n",
    "        if test_df:\n",
    "            test_df = test_df.merge(agg_df, how = 'left', on = col)\n",
    "        if valid_df:\n",
    "            valid_df = valid_df.merge(agg_df, how = 'left', on = col)\n",
    "        if localtest_df:\n",
    "            localtest_df = localtest_df.merge(agg_df, how = 'left', on = col)\n",
    "\n",
    "    return train_df, test_df, valid_df, localtest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557          5          2          1   \n",
       "1         2017-01-01  C_ID_3d0044924f          4          1          0   \n",
       "2         2016-08-01  C_ID_d639edf6cd          2          2          0   \n",
       "3         2017-09-01  C_ID_186d6a6901          4          3          0   \n",
       "4         2017-11-01  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  \n",
       "0 -0.820283  \n",
       "1  0.392913  \n",
       "2  0.688056  \n",
       "3  0.142495  \n",
       "4 -0.159749  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_agg_1 = pd.read_pickle(HIST_AGG_FILE_1)\n",
    "hist_agg_2 = pd.read_pickle(HIST_AGG_FILE_2)\n",
    "new_agg_1 = pd.read_pickle(NEW_AGG_FILE_1)\n",
    "new_agg_2 = pd.read_pickle(NEW_AGG_FILE_2)\n",
    "merch = pd.read_csv(MERCHANTS_FILE)\n",
    "train = pd.read_csv(TRAIN_FILE, parse_dates = [\"first_active_month\"])\n",
    "test = pd.read_csv(TEST_FILE, parse_dates = [\"first_active_month\"])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformations_and_merges_eliminating_nans(df = train, merch_df = merch.copy(), hist_agg_1_df = hist_agg_1, \n",
    "                                                     hist_agg_2_df = hist_agg_2, new_agg_1_df = new_agg_1, \n",
    "                                                     new_agg_2_df = new_agg_2, istrain = True):\n",
    "    \n",
    "    # card_id is a hexadecimal number. Convert it into decimal.\n",
    "    df['card_id'] = df['card_id'].apply(lambda s : int(s[5:], 16))\n",
    "    df = reduce_mem_usage(df = df, verbose = True)\n",
    "    \n",
    "    if istrain:\n",
    "        # merchant_id is a hexadecimal number. Convert it into decimal.\n",
    "        merch_df['merchant_id'] = merch_df['merchant_id'].apply(lambda s : int(s[5:], 16))\n",
    "    # convert categorical features into numerical ones using one-hot encoding (to avoid nan values)\n",
    "    merch_cat_feats = ['category_1', 'most_recent_sales_range', 'most_recent_purchases_range', 'category_4']\n",
    "    merch_df = pd.get_dummies(merch_df, columns = merch_cat_feats)\n",
    "    \n",
    "    # impute mean for missing values in numerical columns of hist_agg_1, hist_agg_2, new_agg_1, new_agg_2\n",
    "    num_cols_with_nans = [x for x in hist_agg_1_df.columns[hist_agg_1_df.isna().any()] if x not in categorical_features]\n",
    "    hist_agg_1_df[num_cols_with_nans] = hist_agg_1_df[num_cols_with_nans].fillna(hist_agg_1_df[num_cols_with_nans].mean())\n",
    "    \n",
    "    num_cols_with_nans = [x for x in hist_agg_2_df.columns[hist_agg_2.isna().any()] if x not in categorical_features]\n",
    "    hist_agg_2_df[num_cols_with_nans] = hist_agg_2_df[num_cols_with_nans].fillna(hist_agg_2_df[num_cols_with_nans].mean())\n",
    "    \n",
    "    num_cols_with_nans = [x for x in new_agg_1_df.columns[new_agg_1.isna().any()] if x not in categorical_features]\n",
    "    new_agg_1_df[num_cols_with_nans] = new_agg_1_df[num_cols_with_nans].fillna(new_agg_1_df[num_cols_with_nans].mean())\n",
    "    \n",
    "    num_cols_with_nans = [x for x in new_agg_2_df.columns[new_agg_2.isna().any()] if x not in categorical_features]\n",
    "    new_agg_2_df[num_cols_with_nans] = new_agg_2_df[num_cols_with_nans].fillna(new_agg_2_df[num_cols_with_nans].mean())\n",
    "    \n",
    "    # merges\n",
    "    df = df.merge(hist_agg_1_df, how = 'left', on = 'card_id')\n",
    "    df = df.merge(hist_agg_2_df, how = 'left', on = 'card_id')\n",
    "    df = df.merge(new_agg_1_df, how = 'left', on = 'card_id')\n",
    "    df = df.merge(new_agg_2_df, how = 'left', on = 'card_id')\n",
    "    df = df.merge(merch_df, how = 'left', left_on = 'hist_merchant_id_mode_auth_dom_lag2m', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_hist_merchant_id_mode_auth_dom_lag2m'))\n",
    "    df = df.merge(merch_df, how = 'left', left_on = 'hist_merchant_id_mode_auth_nondom_lag2m', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_hist_merchant_id_mode_auth_nondom_lag2m'))\n",
    "    df = df.merge(merch_df, how = 'left', left_on = 'new_merchant_id_mode_dom_all', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_new_merchant_id_mode_dom_all'))\n",
    "    df = df.merge(merch_df, how = 'left', left_on = 'new_merchant_id_mode_nondom_all', right_on = 'merchant_id',\n",
    "                  suffixes = ('', '_new_merchant_id_mode_nondom_all'))\n",
    "    for colname in merch_df.columns:\n",
    "        df.rename(columns = {colname: colname + '_hist_merchant_id_mode_auth_dom_lag2m'}, inplace = True)\n",
    "    df.drop(['merchant_id_hist_merchant_id_mode_auth_dom_lag2m',\n",
    "             'merchant_id_hist_merchant_id_mode_auth_nondom_lag2m',\n",
    "             'merchant_id_new_merchant_id_mode_dom_all',\n",
    "             'merchant_id_new_merchant_id_mode_nondom_all'], inplace = True, axis = 1)\n",
    "    \n",
    "    # drop categorical features as Isolation Forest can't handle them\n",
    "    df.drop(categorical_features, axis = 1, inplace = True)\n",
    "    \n",
    "    df = reduce_mem_usage(df = df, verbose = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  9.24 MB\n",
      "Reduced memory usage:  4.04 MB (56.2% reduction)\n",
      "Starting memory usage: 596.37 MB\n"
     ]
    }
   ],
   "source": [
    "train_df = data_transformations_and_merges_eliminating_nans(df = train)\n",
    "train_df['is_outlier'] = train_df['target'].apply(lambda x : x < -30)\n",
    "train_df['is_outlier'] = train_df['is_outlier'].apply(lambda x : int(x))\n",
    "train_target = train_df['target'].copy()\n",
    "train_df.drop('target', axis = 1, inplace = True)\n",
    "# split into training set (80%) and local test set (20%)\n",
    "X_train, X_localtest, y_train, y_localtest = train_test_split(train_df, train_target, train_size = 0.8, random_state = 1,\n",
    "                                                              stratify = train_df[['feature_1', 'feature_2', 'feature_3', \n",
    "                                                                                   'is_outlier']])\n",
    "y_train_label = X_train['is_outlier'].copy()\n",
    "y_localtest_label = X_localtest['is_outlier'].copy()\n",
    "X_train.drop('is_outlier', axis = 1, inplace = True)\n",
    "X_localtest.drop('is_outlier', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_transformations_and_merges_eliminating_nans(df = test, istrain = False)  # why should I use istrain=False?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_localtest.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((X_train, y_train, y_train_label), open(DIRECTORY + 'localtrain_set_16_notebook.p', 'wb'))\n",
    "pickle.dump((X_localtest, y_localtest, y_localtest_label), open(DIRECTORY + 'localtest_set_16_notebook.p', 'wb'))\n",
    "pickle.dump(X_test, open(DIRECTORY + 'test_set_16_notebook.p', 'wb'))\n",
    "pickle.dump((train_df, train_target), open(DIRECTORY + 'full_train_set_16_notebook.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_target = y_train.min()\n",
    "y_train_label.shape, outlier_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_localtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop = True)\n",
    "X_localtest = X_localtest.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest ----\n",
    "seed = 1\n",
    "contamination_ratio = y_train_label.sum() / y_train_label.shape[0]\n",
    "print('Contamination:', contamination_ratio)\n",
    "\n",
    "# training the model\n",
    "clf = IsolationForest(n_estimators = 100, max_samples = 0.02, contamination = contamination_ratio, n_jobs = -1, \n",
    "                      random_state = seed, verbose = 10)\n",
    "clf.fit(X_train.values)\n",
    "\n",
    "# Predictions should be transformed as Isolation Forest predicts 1 for outliers, -1 for inliers,\n",
    "# but our labels are 1 for outliers, 0 for inliers\n",
    "y_pred_train = (clf.predict(X_train.values) + 1) / 2\n",
    "y_pred_localtest = (clf.predict(X_localtest.values) + 1) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on localtest\n",
    "print(\"Accuracy on localtest:\", accuracy_score(y_localtest_label, y_pred_localtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inliers in localtest\n",
    "print(\"Accuracy on inliers:\", accuracy_score(y_localtest_label[y_localtest_label == 0], \n",
    "                                             y_pred_localtest[y_localtest_label == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers in localtest\n",
    "print(\"Accuracy on inliers:\", accuracy_score(y_localtest_label[y_localtest_label == 1], \n",
    "                                             y_pred_localtest[y_localtest_label == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_localtest_label, y_pred_localtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = (clf.predict(X_test.values) + 1) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of positive predictions\n",
    "sum(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test[y_pred_test == 1] = outlier_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "submission['target'] = y_pred_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 0\n",
    "submission.to_csv(SUBMIT_FILENAME + str(NUM) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features before training Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(np_array):\n",
    "    '''Normalizes a numpy 2d array columnwise'''\n",
    "    x = np_array - np_array.mean(axis = 0)\n",
    "    x = x / np_array.std(axis = 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train = normalize(X_train.values)\n",
    "norm_localtest = normalize(X_localtest.values)\n",
    "norm_test = normalize(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest ----\n",
    "seed = 1\n",
    "\n",
    "# training the model\n",
    "clf_norm = IsolationForest(n_estimators = 100, max_samples = 0.02, contamination = contamination_ratio, n_jobs = -1, \n",
    "                      random_state = seed, verbose = 10)\n",
    "clf_norm.fit(norm_train)\n",
    "\n",
    "# Predictions should be transformed as Isolation Forest predicts 1 for outliers, -1 for inliers,\n",
    "# but our labels are 1 for outliers, 0 for inliers\n",
    "y_pred_train = (clf_norm.predict(norm_train) + 1) / 2\n",
    "y_pred_localtest = (clf_norm.predict(norm_localtest) + 1) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy on localtest\n",
    "print(\"Accuracy on localtest:\", accuracy_score(y_localtest_label, y_pred_localtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inliers in localtest\n",
    "print(\"Accuracy on inliers:\", accuracy_score(y_localtest_label[y_localtest_label == 0], \n",
    "                                             y_pred_localtest[y_localtest_label == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers in localtest\n",
    "print(\"Accuracy on inliers:\", accuracy_score(y_localtest_label[y_localtest_label == 1], \n",
    "                                             y_pred_localtest[y_localtest_label == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_localtest_label, y_pred_localtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for normalized test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = (clf.predict(norm_test) + 1) /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of positive predictions\n",
    "sum(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test[y_pred_test == 1] = outlier_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(SAMPLE_SUBMISSION_FILE)\n",
    "submission['target'] = y_pred_test\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 1\n",
    "submission.to_csv(SUBMIT_FILENAME + str(NUM) + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
